{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AvitoTech: Загрузка реальных данных из MinIO → CORE\n",
    "\n",
    "**Источник**: `s3a://raw/avitotech/<dataset>/ingest_date=2026-02-14/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark инициализирован\n"
     ]
    }
   ],
   "source": [
    "# Инициализация Spark с поддержкой MinIO (S3)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, coalesce, from_unixtime\n",
    "from pyspark.sql.functions import broadcast \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, TimestampType, DateType\n",
    "import psycopg2\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .appName(\"AvitoTech Real Data Load\") \n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"airflow\") \n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"airflow123\") \n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \n",
    "    .config(\"spark.jars\", \"/jars/hadoop-aws-3.3.4.jar,/jars/aws-java-sdk-bundle-1.12.262.jar,/jars/postgresql-42.7.4.jar\") \n",
    "    .config(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\")\n",
    "    .config(\"spark.sql.parquet.timestampNTZ.enabled\", \"true\")\n",
    "    .config(\"spark.sql.legacy.parquet.nanosAsLong\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.default.parallelism\", \"100\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.7\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Spark инициализирован\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INGEST_DATE = \"2026-02-14\"\n",
    "JDBC_URL = \"jdbc:postgresql://postgres:5432/dwh\"\n",
    "JDBC_PROPS = {\n",
    "    \"user\": \"app\",\n",
    "    \"password\": \"app\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 330:=================================================>    (92 + 6) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clickstream rows: 68,806,152\n",
      "events rows: 19\n",
      "cat_features rows: 22,646,691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Загрузка данных из MinIO\n",
    "# Схема для clickstream (с правильным типом event_date)\n",
    "clickstream_schema = StructType([\n",
    "    StructField(\"cookie\", LongType(), True),\n",
    "    StructField(\"item\", LongType(), True),\n",
    "    StructField(\"event\", LongType(), True),\n",
    "    StructField(\"event_date\", TimestampType(), True),  # Критично: не datetime[ns], а TimestampType\n",
    "    StructField(\"surface\", LongType(), True),\n",
    "    StructField(\"platform\", LongType(), True),\n",
    "    StructField(\"node\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Схема для events\n",
    "events_schema = StructType([\n",
    "    StructField(\"event\", LongType(), True),\n",
    "    StructField(\"is_contact\", IntegerType(), True)  # 0/1 как целое число\n",
    "])\n",
    "\n",
    "# Схема для cat_features\n",
    "cat_features_schema = StructType([\n",
    "    StructField(\"item\", LongType(), True),\n",
    "    StructField(\"location\", LongType(), True),\n",
    "    StructField(\"category\", LongType(), True),\n",
    "    StructField(\"clean_params\", StringType(), True),  # JSON-строка\n",
    "    StructField(\"node\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Ячейка 3: Загрузка данных\n",
    "df_click = spark.read \\\n",
    "    .option(\"datetimeRebaseMode\", \"CORRECTED\") \\\n",
    "    .option(\"int96RebaseMode\", \"CORRECTED\") \\\n",
    "    .parquet(f\"s3a://raw/avitotech/clickstream/ingest_date={INGEST_DATE}/\") \\\n",
    "    .repartition(50)\n",
    "\n",
    "df_events = spark.read.parquet(f\"s3a://raw/avitotech/events/ingest_date={INGEST_DATE}/\")\n",
    "df_cats = spark.read \\\n",
    "    .option(\"datetimeRebaseMode\", \"CORRECTED\") \\\n",
    "    .option(\"int96RebaseMode\", \"CORRECTED\") \\\n",
    "    .parquet(f\"s3a://raw/avitotech/cat_features/ingest_date={INGEST_DATE}/\") \\\n",
    "    .repartition(50)\n",
    "\n",
    "# Получаем количество строк\n",
    "click_count = df_click.count()\n",
    "events_count = df_events.count()\n",
    "cats_count = df_cats.count()\n",
    "\n",
    "print(f\"clickstream rows: {click_count:,}\")\n",
    "print(f\"events rows: {events_count:,}\")\n",
    "print(f\"cat_features rows: {cats_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 347:=====================================================>(99 + 1) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched facts: 68,806,152 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Джойним события с флагом контакта и категориями/локациями\n",
    "df_enriched = df_click \\\n",
    "    .join(broadcast(df_events), \"event\", \"left\") \\\n",
    "    .join(df_cats.select(\"item\", \"category\", \"location\", \"node\"), \"item\", \"left\") \\\n",
    "    .withColumn(\"ingest_date\", lit(INGEST_DATE).cast(DateType())) \\\n",
    "    .withColumn(\"event_date\", (col(\"event_date\") / 1000000000).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_contact\", col(\"is_contact\").cast(\"boolean\"))\n",
    "\n",
    "enriched_count = df_enriched.count()\n",
    "print(f\"Enriched facts: {enriched_count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалены старые записи за 2026-02-14\n",
      "Схема DataFrame перед записью в PostgreSQL:\n",
      "root\n",
      " |-- cookie: long (nullable = true)\n",
      " |-- item: long (nullable = true)\n",
      " |-- event_date: timestamp (nullable = true)\n",
      " |-- surface_id: long (nullable = true)\n",
      " |-- platform_id: long (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- location_id: long (nullable = true)\n",
      " |-- is_contact: boolean (nullable = true)\n",
      " |-- ingest_date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 365:====================================================> (98 + 2) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Факты загружены в core.fct_user_interactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Идемпотентная загрузка в фактовую таблицу: удаляем старые данные за эту дату через psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    host=\"postgres\",\n",
    "    database=\"dwh\",\n",
    "    user=JDBC_PROPS[\"user\"],\n",
    "    password=JDBC_PROPS[\"password\"]\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(f\"DELETE FROM core.fct_user_interactions WHERE ingest_date = '{INGEST_DATE}'\")\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(f\"Удалены старые записи за {INGEST_DATE}\")\n",
    "\n",
    "# Вставляем новые данные\n",
    "df_to_insert = df_enriched.select(\n",
    "    \"cookie\", \"item\", \"event_date\", \"surface\", \"platform\",\n",
    "    \"category\", \"location\", \"is_contact\", \"ingest_date\"\n",
    ").withColumnRenamed(\"surface\", \"surface_id\") \\\n",
    " .withColumnRenamed(\"platform\", \"platform_id\") \\\n",
    " .withColumnRenamed(\"category\", \"category_id\") \\\n",
    " .withColumnRenamed(\"location\", \"location_id\")\n",
    "\n",
    "# Проверим схему перед записью\n",
    "print(\"Схема DataFrame перед записью в PostgreSQL:\")\n",
    "df_to_insert.printSchema()\n",
    "\n",
    "# Запись в PostgreSQL\n",
    "df_to_insert.write \\\n",
    "    .jdbc(JDBC_URL, \"core.fct_user_interactions\", mode=\"append\", properties=JDBC_PROPS)\n",
    "\n",
    "print(\"✅ Факты загружены в core.fct_user_interactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ЗАГРУЗКА ЗАВЕРШЕНА\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 400:=====================================================>(99 + 1) / 100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фактов загружено: 68,806,152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Финальная проверка\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ЗАГРУЗКА ЗАВЕРШЕНА\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Фактов загружено: {df_enriched.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
